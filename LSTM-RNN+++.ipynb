{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm 内部结构 源代码（成功2019-8-5）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "def sigmoid(x): \n",
    "    return 1. / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(values): \n",
    "    return values*(1-values)\n",
    "\n",
    "def tanh_derivative(values): \n",
    "    return 1. - values ** 2\n",
    "\n",
    "# createst uniform random array w/ values in [a,b) and shape args\n",
    "def rand_arr(a, b, *args): \n",
    "    np.random.seed(0)\n",
    "    return np.random.rand(*args) * (b - a) + a\n",
    "\n",
    "class LstmParam:\n",
    "    def __init__(self, mem_cell_ct, x_dim):\n",
    "        self.mem_cell_ct = mem_cell_ct\n",
    "        self.x_dim = x_dim\n",
    "        concat_len = x_dim + mem_cell_ct\n",
    "        # weight matrices\n",
    "        self.wg = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
    "        self.wi = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len) \n",
    "        self.wf = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
    "        self.wo = rand_arr(-0.1, 0.1, mem_cell_ct, concat_len)\n",
    "        # bias terms\n",
    "        self.bg = rand_arr(-0.1, 0.1, mem_cell_ct) \n",
    "        self.bi = rand_arr(-0.1, 0.1, mem_cell_ct) \n",
    "        self.bf = rand_arr(-0.1, 0.1, mem_cell_ct) \n",
    "        self.bo = rand_arr(-0.1, 0.1, mem_cell_ct) \n",
    "        # diffs (derivative of loss function w.r.t. all parameters)\n",
    "        self.wg_diff = np.zeros((mem_cell_ct, concat_len)) \n",
    "        self.wi_diff = np.zeros((mem_cell_ct, concat_len)) \n",
    "        self.wf_diff = np.zeros((mem_cell_ct, concat_len)) \n",
    "        self.wo_diff = np.zeros((mem_cell_ct, concat_len)) \n",
    "        self.bg_diff = np.zeros(mem_cell_ct) \n",
    "        self.bi_diff = np.zeros(mem_cell_ct) \n",
    "        self.bf_diff = np.zeros(mem_cell_ct) \n",
    "        self.bo_diff = np.zeros(mem_cell_ct) \n",
    "\n",
    "    def apply_diff(self, lr = 1):\n",
    "        self.wg -= lr * self.wg_diff\n",
    "        self.wi -= lr * self.wi_diff\n",
    "        self.wf -= lr * self.wf_diff\n",
    "        self.wo -= lr * self.wo_diff\n",
    "        self.bg -= lr * self.bg_diff\n",
    "        self.bi -= lr * self.bi_diff\n",
    "        self.bf -= lr * self.bf_diff\n",
    "        self.bo -= lr * self.bo_diff\n",
    "        # reset diffs to zero\n",
    "        self.wg_diff = np.zeros_like(self.wg)\n",
    "        self.wi_diff = np.zeros_like(self.wi) \n",
    "        self.wf_diff = np.zeros_like(self.wf) \n",
    "        self.wo_diff = np.zeros_like(self.wo) \n",
    "        self.bg_diff = np.zeros_like(self.bg)\n",
    "        self.bi_diff = np.zeros_like(self.bi) \n",
    "        self.bf_diff = np.zeros_like(self.bf) \n",
    "        self.bo_diff = np.zeros_like(self.bo) \n",
    "\n",
    "class LstmState:\n",
    "    def __init__(self, mem_cell_ct, x_dim):\n",
    "        self.g = np.zeros(mem_cell_ct)\n",
    "        self.i = np.zeros(mem_cell_ct)\n",
    "        self.f = np.zeros(mem_cell_ct)\n",
    "        self.o = np.zeros(mem_cell_ct)\n",
    "        self.s = np.zeros(mem_cell_ct)\n",
    "        self.h = np.zeros(mem_cell_ct)\n",
    "        self.bottom_diff_h = np.zeros_like(self.h)\n",
    "        self.bottom_diff_s = np.zeros_like(self.s)\n",
    "    \n",
    "class LstmNode:\n",
    "    def __init__(self, lstm_param, lstm_state):\n",
    "        # store reference to parameters and to activations\n",
    "        self.state = lstm_state\n",
    "        self.param = lstm_param\n",
    "        # non-recurrent input concatenated with recurrent input\n",
    "        self.xc = None\n",
    "\n",
    "    def bottom_data_is(self, x, s_prev = None, h_prev = None):\n",
    "        # if this is the first lstm node in the network\n",
    "        if s_prev is None: s_prev = np.zeros_like(self.state.s)\n",
    "        if h_prev is None: h_prev = np.zeros_like(self.state.h)\n",
    "        # save data for use in backprop\n",
    "        self.s_prev = s_prev\n",
    "        self.h_prev = h_prev\n",
    "\n",
    "        # concatenate x(t) and h(t-1)\n",
    "        xc = np.hstack((x,  h_prev))\n",
    "        self.state.g = np.tanh(np.dot(self.param.wg, xc) + self.param.bg)\n",
    "        self.state.i = sigmoid(np.dot(self.param.wi, xc) + self.param.bi)\n",
    "        self.state.f = sigmoid(np.dot(self.param.wf, xc) + self.param.bf)\n",
    "        self.state.o = sigmoid(np.dot(self.param.wo, xc) + self.param.bo)\n",
    "        self.state.s = self.state.g * self.state.i + s_prev * self.state.f\n",
    "        self.state.h = self.state.s * self.state.o\n",
    "\n",
    "        self.xc = xc\n",
    "    \n",
    "    def top_diff_is(self, top_diff_h, top_diff_s):\n",
    "        # notice that top_diff_s is carried along the constant error carousel\n",
    "        ds = self.state.o * top_diff_h + top_diff_s\n",
    "        do = self.state.s * top_diff_h\n",
    "        di = self.state.g * ds\n",
    "        dg = self.state.i * ds\n",
    "        df = self.s_prev * ds\n",
    "\n",
    "        # diffs w.r.t. vector inside sigma / tanh function\n",
    "        di_input = sigmoid_derivative(self.state.i) * di \n",
    "        df_input = sigmoid_derivative(self.state.f) * df \n",
    "        do_input = sigmoid_derivative(self.state.o) * do \n",
    "        dg_input = tanh_derivative(self.state.g) * dg\n",
    "\n",
    "        # diffs w.r.t. inputs\n",
    "        self.param.wi_diff += np.outer(di_input, self.xc)\n",
    "        self.param.wf_diff += np.outer(df_input, self.xc)\n",
    "        self.param.wo_diff += np.outer(do_input, self.xc)\n",
    "        self.param.wg_diff += np.outer(dg_input, self.xc)\n",
    "        self.param.bi_diff += di_input\n",
    "        self.param.bf_diff += df_input       \n",
    "        self.param.bo_diff += do_input\n",
    "        self.param.bg_diff += dg_input       \n",
    "\n",
    "        # compute bottom diff\n",
    "        dxc = np.zeros_like(self.xc)\n",
    "        dxc += np.dot(self.param.wi.T, di_input)\n",
    "        dxc += np.dot(self.param.wf.T, df_input)\n",
    "        dxc += np.dot(self.param.wo.T, do_input)\n",
    "        dxc += np.dot(self.param.wg.T, dg_input)\n",
    "\n",
    "        # save bottom diffs\n",
    "        self.state.bottom_diff_s = ds * self.state.f\n",
    "        self.state.bottom_diff_h = dxc[self.param.x_dim:]\n",
    "\n",
    "class LstmNetwork():\n",
    "    def __init__(self, lstm_param):\n",
    "        self.lstm_param = lstm_param\n",
    "        self.lstm_node_list = []\n",
    "        # input sequence\n",
    "        self.x_list = []\n",
    "\n",
    "    def y_list_is(self, y_list, loss_layer):\n",
    "        \"\"\"\n",
    "        Updates diffs by setting target sequence \n",
    "        with corresponding loss layer. \n",
    "        Will *NOT* update parameters.  To update parameters,\n",
    "        call self.lstm_param.apply_diff()\n",
    "        \"\"\"\n",
    "        assert len(y_list) == len(self.x_list)\n",
    "        idx = len(self.x_list) - 1\n",
    "        # first node only gets diffs from label ...\n",
    "        loss = loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "        diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "        # here s is not affecting loss due to h(t+1), hence we set equal to zero\n",
    "        diff_s = np.zeros(self.lstm_param.mem_cell_ct)\n",
    "        self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)\n",
    "        idx -= 1\n",
    "\n",
    "        ### ... following nodes also get diffs from next nodes, hence we add diffs to diff_h\n",
    "        ### we also propagate error along constant error carousel using diff_s\n",
    "        while idx >= 0:\n",
    "            loss += loss_layer.loss(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "            diff_h = loss_layer.bottom_diff(self.lstm_node_list[idx].state.h, y_list[idx])\n",
    "            diff_h += self.lstm_node_list[idx + 1].state.bottom_diff_h\n",
    "            diff_s = self.lstm_node_list[idx + 1].state.bottom_diff_s\n",
    "            self.lstm_node_list[idx].top_diff_is(diff_h, diff_s)\n",
    "            idx -= 1 \n",
    "\n",
    "        return loss\n",
    "\n",
    "    def x_list_clear(self):\n",
    "        self.x_list = []\n",
    "\n",
    "    def x_list_add(self, x):\n",
    "        self.x_list.append(x)\n",
    "        if len(self.x_list) > len(self.lstm_node_list):\n",
    "            # need to add new lstm node, create new state mem\n",
    "            lstm_state = LstmState(self.lstm_param.mem_cell_ct, self.lstm_param.x_dim)\n",
    "            self.lstm_node_list.append(LstmNode(self.lstm_param, lstm_state))\n",
    "\n",
    "        # get index of most recent x input\n",
    "        idx = len(self.x_list) - 1\n",
    "        if idx == 0:\n",
    "            # no recurrent inputs yet\n",
    "            self.lstm_node_list[idx].bottom_data_is(x)\n",
    "        else:\n",
    "            s_prev = self.lstm_node_list[idx - 1].state.s\n",
    "            h_prev = self.lstm_node_list[idx - 1].state.h\n",
    "            self.lstm_node_list[idx].bottom_data_is(x, s_prev, h_prev)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#from lstm import LstmParam, LstmNetwork\n",
    "\n",
    "\n",
    "class ToyLossLayer:\n",
    "    \"\"\"\n",
    "    Computes square loss with first element of hidden layer array.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def loss(self, pred, label):\n",
    "        return (pred[0] - label) ** 2\n",
    "\n",
    "    @classmethod\n",
    "    def bottom_diff(self, pred, label):\n",
    "        diff = np.zeros_like(pred)\n",
    "        diff[0] = 2 * (pred[0] - label)\n",
    "        return diff\n",
    "\n",
    "\n",
    "def example_0():\n",
    "    # learns to repeat simple sequence from random inputs\n",
    "    np.random.seed(0)\n",
    "\n",
    "    # parameters for input data dimension and lstm cell count\n",
    "    mem_cell_ct = 100\n",
    "    x_dim = 50\n",
    "    lstm_param = LstmParam(mem_cell_ct, x_dim)\n",
    "    lstm_net = LstmNetwork(lstm_param)\n",
    "    y_list = [-0.5, 0.2, 0.1, -0.5]\n",
    "    input_val_arr = [np.random.random(x_dim) for _ in y_list]\n",
    "\n",
    "    for cur_iter in range(100):\n",
    "        print(\"iter\", \"%2s\" % str(cur_iter), end=\": \")\n",
    "        for ind in range(len(y_list)):\n",
    "            lstm_net.x_list_add(input_val_arr[ind])\n",
    "\n",
    "        print(\"y_pred = [\" +\n",
    "              \", \".join([\"% 2.5f\" % lstm_net.lstm_node_list[ind].state.h[0] for ind in range(len(y_list))]) +\n",
    "              \"]\", end=\", \")\n",
    "\n",
    "        loss = lstm_net.y_list_is(y_list, ToyLossLayer)\n",
    "        print(\"loss:\", \"%.3e\" % loss)\n",
    "        lstm_param.apply_diff(lr=0.1)\n",
    "        lstm_net.x_list_clear()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    example_0()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#分解学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameters for input data dimension and lstm cell count\n",
    "mem_cell_ct = 100\n",
    "x_dim = 50\n",
    "lstm_param = LstmParam(mem_cell_ct, x_dim)\n",
    "lstm_net = LstmNetwork(lstm_param)\n",
    "y_list = [-0.5, 0.2, 0.1, -0.5]\n",
    "input_val_arr = [np.random.random(x_dim) for _ in y_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_val_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ToyLossLayer:\n",
    "    \"\"\"\n",
    "    Computes square loss with first element of hidden layer array.\n",
    "    \"\"\"\n",
    "    @classmethod\n",
    "    def loss(self, pred, label):\n",
    "        return (pred[0] - label) ** 2\n",
    "\n",
    "    @classmethod\n",
    "    def bottom_diff(self, pred, label):\n",
    "        diff = np.zeros_like(pred)\n",
    "        diff[0] = 2 * (pred[0] - label)\n",
    "        return diff\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cur_iter in range(2):\n",
    "        print(\"iter\", \"%2s\" % str(cur_iter), end=\": \")\n",
    "        for ind in range(len(y_list)):\n",
    "            lstm_net.x_list_add(input_val_arr[ind])\n",
    "\n",
    "        node_state_h=[\"% 2.5f\" % lstm_net.lstm_node_list[ind].state.h[0] for ind in range(len(y_list))] \n",
    "        print(node_state_h)\n",
    "        print(\"y_pred = [\" + \", \".join(node_state_h) + \"]\", end=\", \")\n",
    "\n",
    "        loss = lstm_net.y_list_is(y_list, ToyLossLayer)\n",
    "        print(\"loss:\", \"%.3e\" % loss)\n",
    "        lstm_param.apply_diff(lr=0.1)\n",
    "        lstm_net.x_list_clear()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_net.lstm_node_list[1].state.h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Numpy实现简单RNN的前向传播"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#一、使用Numpy实现简单RNN的前向传播\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "timesteps = 100\n",
    "input_features = 32\n",
    "output_features = 64\n",
    "\n",
    "# 输入有100个时间点，每个时间点有32维的数据\n",
    "inputs = np.random.random((timesteps,input_features))\n",
    "state_t = np.zeros((output_features,))\n",
    "\n",
    "W = np.random.random((output_features,input_features)) # input的权重\n",
    "U = np.random.random((output_features,output_features)) # state的权重\n",
    "b = np.random.random((output_features,)) # bias\n",
    "\n",
    "successive_outputs = []\n",
    "\n",
    "for input_t in inputs:\n",
    "    # 按timesteps进行迭代\n",
    "    \n",
    "    # output_t是一个64维的向量\n",
    "    output_t = np.tanh(np.dot(W,input_t)+np.dot(U,state_t)+b)\n",
    "    \n",
    "    # 将当前时刻的输出保存到successive_outputs中\n",
    "    successive_outputs.append(output_t)\n",
    "    \n",
    "    # 当前时刻的输出作为下一时刻的state\n",
    "    state_t = output_t\n",
    "    \n",
    "final_output_sequence = np.concatenate(successive_outputs,axis=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_output_sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN例子++++(成功2019-9-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一个隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "basic_cell = tf.contrib.rnn.BasicRNNCell(num_units=n_neurons)\n",
    "\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(basic_cell, X, dtype=tf.float32,\n",
    "                                    sequence_length=seq_length)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})\n",
    "    print(\"outputs_val.shape:\", outputs_val.shape, \"states_val.shape:\", states_val.shape)\n",
    "    print(\"\\n outputs_val:\\n\", outputs_val, \"\\n states_val:\\n\", states_val)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先输入X是一个 [batch_size，step，input_size] = [4，2，3] 的tensor，注意我们这里调用的是BasicRNNCell，只有一层循环网络，outputs是最后一层每个step的输出，它的结构是[batch_size，step，n_neurons] = [4，2，5]，states是每一层的最后那个step的输出，由于本例中，我们的循环网络只有一个隐藏层，所以它就代表这一层的最后那个step的输出，因此它和step的大小是没有关系的，我们的X有4个样本组成，输出神经元大小n_neurons是5，因此states的结构就是[batch_size，n_neurons] = [4，5]，最后我们观察数据，states的每条数据正好就是outputs的最后一个step的输出。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三个隐藏层"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0903 21:13:14.878598 15740 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0903 21:13:14.878598 15740 deprecation.py:323] From <ipython-input-1-5168ce7fc784>:15: BasicRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.SimpleRNNCell, and will be replaced by that in Tensorflow 2.0.\n",
      "W0903 21:13:14.878598 15740 deprecation.py:323] From <ipython-input-1-5168ce7fc784>:16: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
      "W0903 21:13:14.878598 15740 deprecation.py:323] From <ipython-input-1-5168ce7fc784>:18: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "W0903 21:13:15.191180 15740 deprecation.py:506] From C:\\Users\\yuli\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0903 21:13:15.191180 15740 deprecation.py:506] From C:\\Users\\yuli\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:459: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "W0903 21:13:15.323271 15740 deprecation.py:323] From C:\\Users\\yuli\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:244: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outputs_val.shape: Tensor(\"rnn/transpose_1:0\", shape=(?, 2, 5), dtype=float32) \n",
      " states_val.shape: (<tf.Tensor 'rnn/while/Exit_3:0' shape=(?, 5) dtype=float32>, <tf.Tensor 'rnn/while/Exit_4:0' shape=(?, 5) dtype=float32>, <tf.Tensor 'rnn/while/Exit_5:0' shape=(?, 5) dtype=float32>)\n",
      "\n",
      " outputs_val: [[[0.         0.10676921 0.         0.         0.10809741]\n",
      "  [0.         0.64463055 0.         0.         1.6826782 ]]\n",
      "\n",
      " [[0.         0.3267716  0.         0.         0.58085716]\n",
      "  [0.         0.         0.         0.         0.        ]]\n",
      "\n",
      " [[0.         0.5901417  0.         0.         1.1638079 ]\n",
      "  [0.         0.13024622 0.         0.         1.877127  ]]\n",
      "\n",
      " [[0.         2.5622442  0.514341   0.         3.8845227 ]\n",
      "  [1.0911777  0.         0.         1.1432415  0.52477217]]] \n",
      " states_val: (array([[5.9811273 , 0.        , 0.        , 1.4522586 , 0.        ],\n",
      "       [2.1700091 , 0.        , 0.        , 1.2011844 , 0.        ],\n",
      "       [5.909309  , 0.        , 0.        , 1.0427554 , 0.        ],\n",
      "       [1.4759719 , 0.31176412, 0.        , 0.        , 0.        ]],\n",
      "      dtype=float32), array([[0.        , 2.4254298 , 2.1884718 , 2.3937278 , 0.        ],\n",
      "       [0.        , 1.2044529 , 0.86860025, 0.5393595 , 0.        ],\n",
      "       [0.        , 1.4824517 , 1.7637056 , 3.1160345 , 0.        ],\n",
      "       [0.4440751 , 0.        , 0.8213698 , 0.        , 1.3457525 ]],\n",
      "      dtype=float32), array([[0.        , 0.64463055, 0.        , 0.        , 1.6826782 ],\n",
      "       [0.        , 0.3267716 , 0.        , 0.        , 0.58085716],\n",
      "       [0.        , 0.13024622, 0.        , 0.        , 1.877127  ],\n",
      "       [1.0911777 , 0.        , 0.        , 1.1432415 , 0.52477217]],\n",
      "      dtype=float32))\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "n_steps = 2\n",
    "n_inputs = 3\n",
    "n_neurons = 5\n",
    "n_layers = 3\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, n_steps, n_inputs])\n",
    "\n",
    "seq_length = tf.placeholder(tf.int32, [None])\n",
    "\n",
    "layers = [tf.contrib.rnn.BasicRNNCell(num_units=n_neurons,\n",
    "                                      activation=tf.nn.relu)\n",
    "          for layer in range(n_layers)]\n",
    "multi_layer_cell = tf.contrib.rnn.MultiRNNCell(layers)\n",
    "\n",
    "outputs, states = tf.nn.dynamic_rnn(multi_layer_cell, X, dtype=tf.float32, sequence_length=seq_length)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "X_batch = np.array([\n",
    "        # step 0     step 1\n",
    "        [[0, 1, 2], [9, 8, 7]], # instance 1\n",
    "        [[3, 4, 5], [0, 0, 0]], # instance 2 (padded with zero vectors)\n",
    "        [[6, 7, 8], [6, 5, 4]], # instance 3\n",
    "        [[9, 0, 1], [3, 2, 1]], # instance 4\n",
    "    ])\n",
    "\n",
    "seq_length_batch = np.array([2, 1, 2, 2])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    outputs_val, states_val = sess.run(\n",
    "        [outputs, states], feed_dict={X: X_batch, seq_length: seq_length_batch})\n",
    "\n",
    "   \n",
    "    print(\"outputs_val.shape:\", outputs, \"\\n states_val.shape:\", states)\n",
    "    print(\"\\n outputs_val:\", outputs_val, \"\\n states_val:\", states_val)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoding:utf-8\n",
    "import tensorflow as tf\n",
    "\n",
    "batch_size=10\n",
    "depth=128 #一个序列中有128个样本\n",
    "\n",
    "inputs=tf.Variable(tf.random_normal([batch_size,depth]))# 一次输入 128 个样本的序列\n",
    "\n",
    "previous_state0=(tf.random_normal([batch_size,100]),tf.random_normal([batch_size,100]))\n",
    "previous_state1=(tf.random_normal([batch_size,200]),tf.random_normal([batch_size,200]))\n",
    "previous_state2=(tf.random_normal([batch_size,300]),tf.random_normal([batch_size,300]))\n",
    "\n",
    "num_units=[100,200,300]\n",
    "print(inputs)\n",
    "\n",
    "cells=[tf.nn.rnn_cell.BasicLSTMCell(num_unit) for num_unit in num_units]\n",
    "mul_cells=tf.nn.rnn_cell.MultiRNNCell(cells)\n",
    "\n",
    "outputs,states=mul_cells(inputs,(previous_state0,previous_state1,previous_state2))\n",
    "\n",
    "print(outputs.shape) #(10, 300)\n",
    "print(states[0]) #第一层LSTM\n",
    "print(states[1]) #第二层LSTM\n",
    "print(states[2]) ##第三层LSTM\n",
    "print(states[0].h.shape) #第一层LSTM的h状态,(10, 100)\n",
    "print(states[0].c.shape) #第一层LSTM的c状态,(10, 100)\n",
    "print(states[1].h.shape) #第二层LSTM的h状态,(10, 200)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.keras.layers.SimpleRNNCell学习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single layer RNN Cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "x = tf.random.normal([4, 80, 100])\n",
    "ht0 = x[:, 0, :]\n",
    "\n",
    "cell = tf.keras.layers.SimpleRNNCell(64)\n",
    "\n",
    "out, ht1 = cell(ht0, [tf.zeros([4, 64])])\n",
    "\n",
    "print(out.shape, ht1[0].shape)\n",
    "#[]\n",
    "#\n",
    "#(TensorShape([4, 64]), TensorShape([4, 64]))\n",
    "id(out), id(ht1[0])  # same id\n",
    "#(4877125168, 4877125168)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ht0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.keras.layers.SimpleRNNCell.__doc__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Layers RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.random.normal([4, 80, 100])\n",
    "ht0 = x[:, 0, :]\n",
    "\n",
    "cell0 = tf.keras.layers.SimpleRNNCell(64)\n",
    "cell2 = tf.keras.layers.SimpleRNNCell(64)\n",
    "state0 = [tf.zeros([4, 64])]\n",
    "state1 = [tf.zeros([4, 64])]\n",
    "\n",
    "out0, state0 = cell0(ht0, state0)\n",
    "out2, state2 = cell2(out0, state1)\n",
    "\n",
    "print(out2.shape, state2[0].shape)\n",
    "#(TensorShape([4, 64]), TensorShape([4, 64]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tf.nn.rnn_cell.BasicRNNCell学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def basic_rnn_demo():\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(num_units=4) ####\n",
    "    zero_state = cell.zero_state(batch_size=2, dtype=tf.float32)\n",
    "    a = tf.random_normal([2, 3, 4])\n",
    "    out, state = tf.nn.dynamic_rnn(\n",
    "        cell=cell,\n",
    "        initial_state=zero_state,\n",
    "        inputs=a\n",
    "    )\n",
    "    print(out)\n",
    "    print(state)\n",
    "    \n",
    "basic_rnn_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "def basic_rnn_demo():\n",
    "    cell = tf.keras.layers.SimpleRNNCell(num_units=4)####\n",
    "    zero_state = cell.zero_state(batch_size=2, dtype=tf.float32)\n",
    "    a = tf.random_normal([2, 3, 4])\n",
    "    out, state = tf.nn.dynamic_rnn(\n",
    "        cell=cell,\n",
    "        initial_state=zero_state,\n",
    "        inputs=a\n",
    "    )\n",
    "    print(out)\n",
    "    print(state)\n",
    "    \n",
    "basic_rnn_demo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "# author: adowu\n",
    "# https://raw.githubusercontent.com/adowu/ado-tensorflow-models/master/03_AllRNN/basic_rnn_demo.py\n",
    "import tensorflow as tf\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "\n",
    "\n",
    "def basic_rnn_demo():\n",
    "    \"\"\"\n",
    "    Most basic rnn\n",
    "    tanh(W * input + U * state + B)\n",
    "\n",
    "    #   inputs [2,3,4]  state = [2,4]\n",
    "    #   unstack(inputs) = 3 size [2,4]\n",
    "    #   每次是进行一个batch的时间步骤的计算，第一个进行的就是每个batch中的第一个字\n",
    "    #   a = [2, 8]\n",
    "    a = concat([inputs,state], 1)\n",
    "    #   kernel_ = [8, 4] kernel_在每个时间步都是共享的\n",
    "    kernel_ = [inputs_dim + num_units,num_units]\n",
    "    #   b = [2, 4]\n",
    "    b = matmul(a, kernel_)\n",
    "    #   c = [2, 4] bias初始化为0 bias 在每个时间步都是共享的\n",
    "    c = b + bias\n",
    "    #   [2, 4]\n",
    "    #   会返回一个tuple，内容都是output，一个作为此时刻的state,这样state就可以了从第一个一直往后更新\n",
    "    output,output = tanh(c)\n",
    "    \"\"\"\n",
    "\n",
    "    cell = tf.nn.rnn_cell.BasicRNNCell(num_units=4)\n",
    "    \n",
    "    zero_state = cell.zero_state(batch_size=2, dtype=tf.float32)\n",
    "    \n",
    "    a = tf.random_normal([2, 3, 4])\n",
    "\n",
    "    \"\"\"\n",
    "    out\n",
    "    tf.Tensor(\n",
    "    [[[ 0.7875833   0.11634824  0.31249827  0.11648687]\n",
    "      [ 0.6418752  -0.9281747   0.6534868   0.3821376 ]\n",
    "      [ 0.9750985  -0.40439364  0.9770327   0.8529797 ]]\n",
    "    \n",
    "     [[-0.09945039 -0.49678802 -0.32603818  0.20098403]\n",
    "      [-0.57557577  0.15389016 -0.7197561  -0.36572933]\n",
    "      [ 0.4485007  -0.51780844 -0.6015551   0.16041796]]], shape=(2, 3, 4), dtype=float32)\n",
    "    \n",
    "    state\n",
    "    tf.Tensor(\n",
    "    [[ 0.9750985  -0.40439364  0.9770327   0.8529797 ]\n",
    "     [ 0.4485007  -0.51780844 -0.6015551   0.16041796]], shape=(2, 4), dtype=float32)\n",
    "    \"\"\"\n",
    "    #   output shape = [2,3,4] 表示的是每个时间步的输出\n",
    "    #   state shape = [2, 4] 表示最后的状态输出\n",
    "    out, state = tf.nn.dynamic_rnn(\n",
    "        cell=cell,\n",
    "        initial_state=zero_state,\n",
    "        inputs=a\n",
    "    )\n",
    "\n",
    "    print(out)\n",
    "    print(state)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    basic_rnn_demo()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN -手写数据集 成功2019-8-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,SimpleRNN,Activation\n",
    "from keras.datasets import mnist\n",
    "from keras.utils import np_utils\n",
    "from keras.optimizers import Adam\n",
    "import numpy as np\n",
    "\n",
    "#(x_train,y_train),(x_test,y_test) = mnist.load_data()\n",
    "\n",
    "\n",
    "#已经下载了数据要，直接加载\n",
    "f = np.load(\"mnist.npz\")\n",
    "x_train, y_train = f['x_train'], f['y_train']\n",
    "x_test, y_test = f['x_test'], f['y_test']\n",
    "f.close()\n",
    "\n",
    "\n",
    "x_train = x_train.reshape(-1,28,28)/255\n",
    "x_test = x_test.reshape(-1,28,28)/255\n",
    "y_train = np_utils.to_categorical(y_train,num_classes=10)\n",
    "y_test = np_utils.to_categorical(y_test,num_classes=10)\n",
    "\n",
    "TIME_STEPS = 28 # as same as the image height\n",
    "INPUT_SIZE = 28 # as same as the image width\n",
    "BATCH_SIZE = 100\n",
    "BATCH_INDEX = 0\n",
    "OUTPUT_SIZE = 10\n",
    "CELL_SIZE = 50 # how many hidden layer\n",
    "LR = 0.001\n",
    "\n",
    "# built the RNN model\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(batch_input_shape=(None,TIME_STEPS,INPUT_SIZE),\n",
    "                    output_dim=CELL_SIZE,\n",
    "                    activation='relu'))\n",
    "model.add(Dense(OUTPUT_SIZE))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "\n",
    "adam = Adam(LR)\n",
    "model.compile(optimizer=adam,loss='categorical_crossentropy',metrics=['accuracy'])\n",
    "\n",
    "# training\n",
    "#print('training...')\n",
    "for step in range(4001):\n",
    "    \n",
    "    x_batch = x_train[BATCH_INDEX:BATCH_SIZE+BATCH_INDEX,:,:]\n",
    "    y_batch = y_train[BATCH_INDEX:BATCH_SIZE+BATCH_INDEX,:]\n",
    "    \n",
    "    cost = model.train_on_batch(x_batch,y_batch)\n",
    "    \n",
    "    BATCH_INDEX += BATCH_SIZE\n",
    "    if BATCH_INDEX >= x_train.shape[0]:\n",
    "        BATCH_INDEX = 0\n",
    "    \n",
    "    if step%500 == 0:\n",
    "        cost,accuracy = model.evaluate(x_test,y_test,batch_size=y_test.shape[0],verbose=False)\n",
    "        print('cost : ',cost,' accuracy : ',accuracy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# 手写Bi-RNN 成功2019--8-1\n",
    "\n",
    "\n",
    "- Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BiRNN Overview\n",
    "\n",
    "<img src=\"https://ai2-s2-public.s3.amazonaws.com/figures/2016-11-08/191dd7df9cb91ac22f56ed0dfa4a5651e8767a51/1-Figure2-1.png\" alt=\"nn\" style=\"width: 600px;\"/>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib import rnn\n",
    "import numpy as np\n",
    "\n",
    "# Import MNIST data\n",
    "#from tensorflow.examples.tutorials.mnist import input_data\n",
    "#mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)\n",
    "\n",
    "# Import MNIST data\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "MNIST_data =r'C:\\Users\\yuli\\[Python程序设计]++++++++++++\\MNIST_data'\n",
    "mnist = input_data.read_data_sets(MNIST_data,one_hot=True) \n",
    "\n",
    "\n",
    "# Training Parameters\n",
    "learning_rate = 0.01\n",
    "training_steps = 100\n",
    "batch_size = 128\n",
    "display_step = 10\n",
    "\n",
    "# Network Parameters\n",
    "num_input = 28 # MNIST data input (img shape: 28*28)\n",
    "timesteps = 28 # timesteps\n",
    "num_hidden = 128 # hidden layer num of features\n",
    "num_classes = 10 # MNIST total classes (0-9 digits)\n",
    "\n",
    "# tf Graph input\n",
    "X = tf.placeholder(\"float\", [None, timesteps, num_input])\n",
    "Y = tf.placeholder(\"float\", [None, num_classes])\n",
    "\n",
    "# Define weights\n",
    "weights = {\n",
    "    # Hidden layer weights => 2*n_hidden because of forward + backward cells\n",
    "    'out': tf.Variable(tf.random_normal([2*num_hidden, num_classes]))}\n",
    "\n",
    "biases = {'out': tf.Variable(tf.random_normal([num_classes]))}\n",
    "\n",
    "\n",
    "def BiRNN(x, weights, biases):\n",
    "\n",
    "    # Prepare data shape to match `rnn` function requirements\n",
    "    # Current data input shape: (batch_size, timesteps, n_input)\n",
    "    # Required shape: 'timesteps' tensors list of shape (batch_size, num_input)\n",
    "\n",
    "    # Unstack to get a list of 'timesteps' tensors of shape (batch_size, num_input)\n",
    "    x = tf.unstack(x, timesteps, 1)\n",
    "\n",
    "    lstm_fw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)#前向\n",
    "    lstm_bw_cell = rnn.BasicLSTMCell(num_hidden, forget_bias=1.0)#后向\n",
    "\n",
    "    # Get lstm cell output\n",
    "    try:\n",
    "        outputs, _, _ = rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,dtype=tf.float32)\n",
    "    except Exception: # Old TensorFlow version only returns outputs not states\n",
    "        outputs =       rnn.static_bidirectional_rnn(lstm_fw_cell, lstm_bw_cell, x,dtype=tf.float32)\n",
    "\n",
    "    # Linear activation, using rnn inner loop last output\n",
    "    return tf.matmul(outputs[-1], weights['out']) + biases['out']\n",
    "\n",
    "logits = BiRNN(X, weights, biases)######################\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "# Evaluate model (with test logits, for dropout to be disabled)\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    sess.run(init)\n",
    "\n",
    "    for step in range(1, training_steps+1):\n",
    "        batch_x, batch_y = mnist.train.next_batch(batch_size)\n",
    "        # Reshape data to get 28 seq of 28 elements\n",
    "        batch_x = batch_x.reshape((batch_size, timesteps, num_input))\n",
    "        \n",
    "        sess.run(train_op, feed_dict={X: batch_x, Y: batch_y})\n",
    "        if step % display_step == 0 or step == 1:\n",
    "            loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                 Y: batch_y})\n",
    "            print(\"Step \" + str(step) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.3f}\".format(acc))\n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 128 mnist test images\n",
    "    test_len = 128\n",
    "    test_data = mnist.test.images[:test_len].reshape((-1, timesteps, num_input))\n",
    "    test_label = mnist.test.labels[:test_len]\n",
    "    print(\"Testing Accuracy:\",sess.run(accuracy, feed_dict={X: test_data, Y: test_label}))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "上面程序运行结果\n",
    "\n",
    "Step 1, Minibatch Loss= 2.6205, Training Accuracy= 0.094\n",
    "Step 10, Minibatch Loss= 2.3016, Training Accuracy= 0.133\n",
    "Step 20, Minibatch Loss= 2.1555, Training Accuracy= 0.250\n",
    "Step 30, Minibatch Loss= 2.1029, Training Accuracy= 0.305\n",
    "Step 40, Minibatch Loss= 2.0525, Training Accuracy= 0.281\n",
    "Step 50, Minibatch Loss= 1.9368, Training Accuracy= 0.398\n",
    "Step 60, Minibatch Loss= 1.8921, Training Accuracy= 0.414\n",
    "Step 70, Minibatch Loss= 1.8578, Training Accuracy= 0.414\n",
    "Step 80, Minibatch Loss= 1.7664, Training Accuracy= 0.422\n",
    "Step 90, Minibatch Loss= 1.7573, Training Accuracy= 0.422\n",
    "Step 100, Minibatch Loss= 1.5477, Training Accuracy= 0.516\n",
    "Optimization Finished!\n",
    "Testing Accuracy: 0.484375"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二进制加法代码（成功）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import copy, numpy as np\n",
    "\n",
    "np.random.seed(0)  # 固定随机数生成器的种子，便于得到固定的输出\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):  # 激活函数\n",
    "    output = 1 / (1 + np.exp(-x))\n",
    "    return output\n",
    "\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):  # 激活函数的导数\n",
    "    return output * (1 - output)\n",
    "\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {}  # 整数到其二进制表示的映射\n",
    "binary_dim = 8  # 暂时制作256以内的加法， 可以调大\n",
    "\n",
    "## 以下5行代码计算0-256的二进制表示\n",
    "largest_number = pow(2, binary_dim)\n",
    "binary = np.unpackbits(np.array([range(largest_number)], dtype=np.uint8).T, axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1  # 学习速率\n",
    "input_dim = 2  # 因为我们是做两个数相加，每次会喂给神经网络两个bit，所以输入的维度是2\n",
    "hidden_dim = 16  # 隐藏层的神经元节点数，远比理论值要大（译者注：理论上而言，应该一个节点就可以记住有无进位了，但我试了发现4的时候都没法收敛），你可以自己调整这个数，看看调大了是容易更快地收敛还是更慢\n",
    "output_dim = 1  # 我们的输出是一个数，所以维度为1\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2 * np.random.random((input_dim, hidden_dim)) - 1  # 输入层到隐藏层的转化矩阵，维度为2*16， 2是输入维度，16是隐藏层维度\n",
    "synapse_1 = 2 * np.random.random((hidden_dim, output_dim)) - 1\n",
    "synapse_h = 2 * np.random.random((hidden_dim, hidden_dim)) - 1\n",
    "# 译者注：np.random.random产生的是[0,1)的随机数，2 * [0, 1) - 1 => [-1, 1)，\n",
    "# 是为了有正有负更快地收敛，这涉及到如何初始化参数的问题，通常来说都是靠“经验”或者说“启发式规则”，说得直白一点就是“蒙的”！机器学习里面，超参数的选择，大部分都是这种情况，哈哈。。。\n",
    "# 我自己试了一下用【0, 2)之间的随机数，貌似不能收敛，用[0,1)就可以，呵呵。。。\n",
    "# 以下三个分别对应三个矩阵的变化\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "# training logic\n",
    "# 学习1000个例子\n",
    "for j in range(1000):\n",
    "\n",
    "    # 下面6行代码，随机产生两个0-128的数字，并查出他们的二进制表示。为了避免相加之和超过256，这里选择两个0-128的数字\n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number / 2)  # int version\n",
    "    a = int2binary[a_int]  # binary encoding\n",
    "    b_int = np.random.randint(largest_number / 2)  # int version\n",
    "    b = int2binary[b_int]  # binary encoding\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]\n",
    "\n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    # 存储神经网络的预测值\n",
    "    d = np.zeros_like(c)\n",
    "    overallError = 0  # 每次把总误差清零\n",
    "\n",
    "    layer_2_deltas = list()  # 存储每个时间点输出层的误差\n",
    "    layer_1_values = list()  # 存储每个时间点隐藏层的值\n",
    "    layer_1_values.append(np.zeros(hidden_dim))  # 一开始没有隐藏层，所以里面都是0\n",
    "\n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim):  # 循环遍历每一个二进制位\n",
    "\n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1], b[binary_dim - position - 1]]])  # 从右到左，每次去两个输入数字的一个bit位\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T  # 正确答案\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X, synapse_0) + np.dot(layer_1_values[-1],\n",
    "                                                        synapse_h))  # （输入层 + 之前的隐藏层） -> 新的隐藏层，这是体现循环神经网络的最核心的地方！！！\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1, synapse_1))  # 隐藏层 * 隐藏层到输出层的转化矩阵synapse_1 -> 输出层\n",
    "        # did we miss?... if so, by how much?\n",
    "        layer_2_error = y - layer_2  # 预测误差是多少\n",
    "        layer_2_deltas.append((layer_2_error) * sigmoid_output_to_derivative(layer_2))  # 我们把每一个时间点的误差导数都记录下来\n",
    "        overallError += np.abs(layer_2_error[0])  # 总误差\n",
    "\n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])  # 记录下每一个预测bit位\n",
    "\n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1))  # 记录下隐藏层的值，在下一个时间点用\n",
    "\n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "\n",
    "    # 前面代码我们完成了所有时间点的正向传播以及计算最后一层的误差，\n",
    "    #现在我们要做的是反向传播，从最后一个时间点到第一个时间点\n",
    "    for position in range(binary_dim):\n",
    "        X = np.array([[a[position], b[position]]])  # 最后一次的两个输入\n",
    "        layer_1 = layer_1_values[-position - 1]      # 当前时间点的隐藏层\n",
    "        prev_layer_1 = layer_1_values[-position - 2]  # 前一个时间点的隐藏层\n",
    "\n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position - 1]  # 当前时间点输出层导数\n",
    "        # error at hidden layer\n",
    "        # 通过后一个时间点（因为是反向传播）的隐藏层误差和当前时间点的输出层误差，计算当前时间点的隐藏层误差\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + layer_2_delta.dot(\n",
    "            synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "        # let's update all our weights so we can try again\n",
    "        # 我们已经完成了当前时间点的反向传播误差计算， 可以构建更新矩阵了。但是我们并不会现在就更新权重矩阵，因为我们还要用他们计算前一个时间点的更新矩阵呢。\n",
    "        # 所以要等到我们完成了所有反向传播误差计算， 才会真正的去更新权重矩阵，我们暂时把更新矩阵存起来。\n",
    "        # 可以看这里了解更多关于反向传播的知识http://iamtrask.github.io/2015/07/12/basic-python-network/\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "\n",
    "        future_layer_1_delta = layer_1_delta\n",
    "\n",
    "    # 我们已经完成了所有的反向传播，可以更新几个转换矩阵了。并把更新矩阵变量清零\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha\n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "\n",
    "    # print out progress\n",
    "    if (j % 1000 == 0):\n",
    "        print(\"Error:\" + str(overallError))\n",
    "        print(\"Pred:\" + str(d))\n",
    "        print(\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index, x in enumerate(reversed(d)):\n",
    "            out += x * pow(2, index)\n",
    "        print(str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print(\"------------\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "运行结果\n",
    "Error:[3.45638663]\n",
    "Pred:[0 0 0 0 0 0 0 1]\n",
    "True:[0 1 0 0 0 1 0 1]\n",
    "9 + 60 = 1\n",
    "------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 三个字母映射代码 成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Naive LSTM to learn three-char time steps to one-char mapping\n",
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))\n",
    "\n",
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print (seq_in, '->', seq_out)\n",
    "\n",
    "print (\"dataX=\",dataX)\n",
    "#dataX= [[0, 1, 2], [1, 2, 3], [2, 3, 4], [3, 4, 5], [4, 5, 6], [5, 6, 7], [6, 7, 8], [7, 8, 9], [8, 9, 10], [9, 10, 11], [10, 11, 12], [11, 12, 13], [12, 13, 14], [13, 14, 15], [14, 15, 16], [15, 16, 17], [16, 17, 18], [17, 18, 19], [18, 19, 20], [19, 20, 21], [20, 21, 22], [21, 22, 23], [22, 23, 24]]\n",
    "\n",
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(  dataX,    ( len(dataX), seq_length, 1 )    )\n",
    "#print ('X=',X)\n",
    "#X= [\n",
    "# [[ 0]\n",
    "#  [ 1]\n",
    "#  [ 2]]\n",
    "#\n",
    "# [[ 1]\n",
    "#  [ 2]\n",
    "#  [ 3]]\n",
    "#\n",
    "# [[ 2]\n",
    "#  [ 3]\n",
    "#  [ 4]]\n",
    "#...]\n",
    "\n",
    "# normalize\n",
    "X = X / float(len(alphabet))\n",
    "#print(\"X=\",X)\n",
    "#X= [[[ 0.        ]\n",
    "#  [ 0.03846154]\n",
    "#  [ 0.07692308]]\n",
    "#...\n",
    "\n",
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n",
    "#print(\"y=\",y)\n",
    "#y= [[ 0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
    "#   0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "# [ 0.  0.  0.  0.  1.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
    "#   0.  0.  0.  0.  0.  0.  0.  0.]\n",
    "#...\n",
    "\n",
    "#  create and fit the model\n",
    "model = Sequential()\n",
    "model.add(     LSTM(32, input_shape=(X.shape[1], X.shape[2]))    )####\n",
    "model.add(     Dense(y.shape[1], activation='softmax')    )\n",
    "model.compile(     loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']   )\n",
    "model.fit(X, y, nb_epoch=50, batch_size=1, verbose=2)\n",
    "\n",
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print (seq_in, \"->\", result)\n",
    "\n",
    "print('End1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下面分解学习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "numpy.random.seed(7)\n",
    "# define the raw dataset\n",
    "alphabet = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "# create mapping of characters to integers (0-25) and the reverse\n",
    "char_to_int = dict((c, i) for i, c in enumerate(alphabet))\n",
    "int_to_char = dict((i, c) for i, c in enumerate(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_to_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare the dataset of input to output pairs encoded as integers\n",
    "seq_length = 3\n",
    "dataX = []\n",
    "dataY = []\n",
    "for i in range(0, len(alphabet) - seq_length, 1):\n",
    "    seq_in = alphabet[i:i + seq_length]\n",
    "    seq_out = alphabet[i + seq_length]\n",
    "    dataX.append([char_to_int[char] for char in seq_in])\n",
    "    dataY.append(char_to_int[seq_out])\n",
    "    print (seq_in, '->', seq_out)\n",
    "\n",
    "print (\"dataX=\",dataX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape X to be [samples, time steps, features]\n",
    "X = numpy.reshape(  dataX,    ( len(dataX), seq_length, 1 )    )\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize\n",
    "X = X / float(len(alphabet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot encode the output variable\n",
    "y = np_utils.to_categorical(dataY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  create and fit the model\n",
    "model = Sequential()\n",
    "model.add(     LSTM(32, input_shape=(X.shape[1], X.shape[2]))    )####\n",
    "model.add(     Dense(y.shape[1], activation='softmax')    )\n",
    "model.compile(     loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy']   )\n",
    "\n",
    "model.fit(X, y, nb_epoch=50, batch_size=1, verbose=2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize performance of the model\n",
    "scores = model.evaluate(X, y, verbose=0)\n",
    "print(\"Model Accuracy: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "# demonstrate some model predictions\n",
    "for pattern in dataX:\n",
    "    x = numpy.reshape(pattern, (1, len(pattern), 1))\n",
    "    x = x / float(len(alphabet))\n",
    "    prediction = model.predict(x, verbose=0)\n",
    "    index = numpy.argmax(prediction)\n",
    "    result = int_to_char[index]\n",
    "    seq_in = [int_to_char[value] for value in pattern]\n",
    "    print (seq_in, \"->\", result)\n",
    "\n",
    "print('End1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 二进制加法代码（成功）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#succeed! Python35\n",
    "\n",
    "import copy, numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "# compute sigmoid nonlinearity\n",
    "def sigmoid(x):\n",
    "    output = 1/(1+np.exp(-x))\n",
    "    return output\n",
    "\n",
    "# convert output of sigmoid function to its derivative\n",
    "def sigmoid_output_to_derivative(output):\n",
    "    return output*(1-output)\n",
    "\n",
    "# training dataset generation\n",
    "int2binary = {} #查找表，这个表是一个实数与对应二进制表示的映射\n",
    "binary_dim = 8 #二进制数的最大长度\n",
    "\n",
    "largest_number = pow(2,binary_dim)  #计算了跟二进制最大长度对应的可以表示的最大十进制数\n",
    "#生成了十进制数转二进制数的查找表，并将其复制到int2binary\n",
    "binary = np.unpackbits(\n",
    "    np.array([range(largest_number)],dtype=np.uint8).T,axis=1)\n",
    "for i in range(largest_number):\n",
    "    int2binary[i] = binary[i]\n",
    "\n",
    "# input variables\n",
    "alpha = 0.1 #学习速率\n",
    "input_dim = 2 #我们要把两个数加起来，所以我们一次要输入两位字符\n",
    "hidden_dim = 16\n",
    "output_dim = 1\n",
    "\n",
    "# initialize neural network weights\n",
    "synapse_0 = 2*np.random.random((input_dim,hidden_dim)) - 1    #2×16\n",
    "synapse_1 = 2*np.random.random((hidden_dim,output_dim)) - 1  #16×1\n",
    "synapse_h = 2*np.random.random((hidden_dim,hidden_dim)) - 1 #16×16\n",
    "\n",
    "synapse_0_update = np.zeros_like(synapse_0)\n",
    "synapse_1_update = np.zeros_like(synapse_1)\n",
    "synapse_h_update = np.zeros_like(synapse_h)\n",
    "\n",
    "\n",
    "# training logic\n",
    "for j in range(10000):\n",
    "    \n",
    "    # generate a simple addition problem (a + b = c)\n",
    "    a_int = np.random.randint(largest_number/2)  # int version\n",
    "    a = int2binary[a_int] # binary encoding a_int对应的二进制表示\n",
    "\n",
    "    b_int = np.random.randint(largest_number/2) # int version\n",
    "    b = int2binary[b_int] # binary encoding\n",
    "\n",
    "    # true answer\n",
    "    c_int = a_int + b_int\n",
    "    c = int2binary[c_int]  #正确结果转化为二进制表示\n",
    "    \n",
    "    # where we'll store our best guess (binary encoded)\n",
    "    d = np.zeros_like(c)  #初始化一个空的二进制数组，用来存储神经网络的预测值\n",
    "\n",
    "    overallError = 0\n",
    "    \n",
    "    layer_2_deltas = list()  #记录layer 2的导数值\n",
    "    layer_1_values = list()  #记录layer 1值\n",
    "    layer_1_values.append(np.zeros(hidden_dim)) #在0时刻是没有之前的隐含层的，所以我们初始化一个全为0的\n",
    "    \n",
    "    # moving along the positions in the binary encoding\n",
    "    for position in range(binary_dim): #循环是遍历二进制数字\n",
    "        \n",
    "        # generate input and output\n",
    "        X = np.array([[a[binary_dim - position - 1],b[binary_dim - position - 1]]]) #X数组中的每个元素包含两个二进制数，其中一个来自a，一个来自b\n",
    "        y = np.array([[c[binary_dim - position - 1]]]).T\n",
    "\n",
    "        # hidden layer (input ~+ prev_hidden)\n",
    "        layer_1 = sigmoid(np.dot(X,synapse_0) + np.dot(layer_1_values[-1],synapse_h))\n",
    "\n",
    "        # output layer (new binary representation)\n",
    "        layer_2 = sigmoid(np.dot(layer_1,synapse_1))\n",
    "\n",
    "        # did we miss?... if so by how much?\n",
    "        layer_2_error = y - layer_2\n",
    "        layer_2_deltas.append((layer_2_error)*sigmoid_output_to_derivative(layer_2)) #把导数值存起来\n",
    "        overallError += np.abs(layer_2_error[0])\n",
    "    \n",
    "        # decode estimate so we can print it out\n",
    "        d[binary_dim - position - 1] = np.round(layer_2[0][0])\n",
    "        \n",
    "        # store hidden layer so we can use it in the next timestep\n",
    "        layer_1_values.append(copy.deepcopy(layer_1)) #将layer_1的值拷贝到另外一个数组里，这样我们就可以下一个时间使用这个值\n",
    "    \n",
    "    future_layer_1_delta = np.zeros(hidden_dim)\n",
    "\n",
    "    #现在我们需要做的就是反向传播\n",
    "    for position in range(binary_dim):\n",
    "        \n",
    "        X = np.array([[a[position],b[position]]])\n",
    "        layer_1 = layer_1_values[-position-1]\n",
    "        prev_layer_1 = layer_1_values[-position-2]\n",
    "        \n",
    "        # error at output layer\n",
    "        layer_2_delta = layer_2_deltas[-position-1]\n",
    "        # error at hidden layer\n",
    "        layer_1_delta = (future_layer_1_delta.dot(synapse_h.T) + \\\n",
    "            layer_2_delta.dot(synapse_1.T)) * sigmoid_output_to_derivative(layer_1)\n",
    "        # let's update all our weights so we can try again\n",
    "        synapse_1_update += np.atleast_2d(layer_1).T.dot(layer_2_delta)\n",
    "        synapse_h_update += np.atleast_2d(prev_layer_1).T.dot(layer_1_delta)\n",
    "        synapse_0_update += X.T.dot(layer_1_delta)\n",
    "        \n",
    "        future_layer_1_delta = layer_1_delta\n",
    "\n",
    "\n",
    "    synapse_0 += synapse_0_update * alpha\n",
    "    synapse_1 += synapse_1_update * alpha\n",
    "    synapse_h += synapse_h_update * alpha    \n",
    "    synapse_0_update *= 0\n",
    "    synapse_1_update *= 0\n",
    "    synapse_h_update *= 0\n",
    "    \n",
    "    # print out progress\n",
    "    if(j % 1000 == 0):\n",
    "        print (\"Error:\" + str(overallError))\n",
    "        print (\"Pred:\" + str(d))\n",
    "        print (\"True:\" + str(c))\n",
    "        out = 0\n",
    "        for index,x in enumerate(reversed(d)):\n",
    "            out += x*pow(2,index)\n",
    "        print (str(a_int) + \" + \" + str(b_int) + \" = \" + str(out))\n",
    "        print (\"------------\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "上面程序运行输出： \n",
    "Error:[ 3.45638663]\n",
    "Pred:[0 0 0 0 0 0 0 1]\n",
    "True:[0 1 0 0 0 1 0 1]\n",
    "9 + 60 = 1\n",
    "------------\n",
    "Error:[ 3.63389116]\n",
    "Pred:[1 1 1 1 1 1 1 1]\n",
    "True:[0 0 1 1 1 1 1 1]\n",
    "28 + 35 = 255\n",
    "------------\n",
    "Error:[ 3.91366595]\n",
    "Pred:[0 1 0 0 1 0 0 0]\n",
    "True:[1 0 1 0 0 0 0 0]\n",
    "116 + 44 = 72\n",
    "------------\n",
    "Error:[ 3.72191702]\n",
    "Pred:[1 1 0 1 1 1 1 1]\n",
    "True:[0 1 0 0 1 1 0 1]\n",
    "4 + 73 = 223\n",
    "------------\n",
    "Error:[ 3.5852713]\n",
    "Pred:[0 0 0 0 1 0 0 0]\n",
    "True:[0 1 0 1 0 0 1 0]\n",
    "71 + 11 = 8\n",
    "------------\n",
    "Error:[ 2.53352328]\n",
    "Pred:[1 0 1 0 0 0 1 0]\n",
    "True:[1 1 0 0 0 0 1 0]\n",
    "81 + 113 = 162\n",
    "------------\n",
    "Error:[ 0.57691441]\n",
    "Pred:[0 1 0 1 0 0 0 1]\n",
    "True:[0 1 0 1 0 0 0 1]\n",
    "81 + 0 = 81\n",
    "------------\n",
    "Error:[ 1.42589952]\n",
    "Pred:[1 0 0 0 0 0 0 1]\n",
    "True:[1 0 0 0 0 0 0 1]\n",
    "4 + 125 = 129\n",
    "------------\n",
    "Error:[ 0.47477457]\n",
    "Pred:[0 0 1 1 1 0 0 0]\n",
    "True:[0 0 1 1 1 0 0 0]\n",
    "39 + 17 = 56\n",
    "------------\n",
    "Error:[ 0.21595037]\n",
    "Pred:[0 0 0 0 1 1 1 0]\n",
    "True:[0 0 0 0 1 1 1 0]\n",
    "11 + 3 = 14\n",
    "------------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 代码 不成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 不成功！\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "#https://blog.csdn.net/bi_diu1368/article/details/90551891\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from tensorflow.contrib import rnn\n",
    "mnist = input_data.read_data_sets('C:\\\\Users\\\\yuli\\\\[机器学习方法]++++++++++++++\\\\MINST_data',one_hot=True)\n",
    "\n",
    "\n",
    "#定义参数\n",
    "\n",
    "#输入一行，一行有28个数据\n",
    "n_input = 28\n",
    "\n",
    "#一共有28行\n",
    "max_time = 28\n",
    "\n",
    "#100个隐藏单元\n",
    "lstm_size = 10\n",
    "\n",
    "#10分类\n",
    "n_class = 10\n",
    "\n",
    "#每批次分50个样本\n",
    "batch_size = 50\n",
    "\n",
    "#一共有n_batch个批次\n",
    "n_batch = int(mnist.train.num_examples/batch_size)\n",
    "\n",
    "\n",
    "#定义输入\n",
    "with tf.name_scope('input'):\n",
    "    x = tf.placeholder(tf.float32,[None,784],name='x-input')\n",
    "    y = tf.placeholder(tf.float32,[None,10],name='y-input')\n",
    "\n",
    "#初始化权重值\n",
    "weights = tf.Variable(tf.truncated_normal([lstm_size,n_class],stddev=0.1))\n",
    "biases = tf.Variable(tf.constant(0.1,shape=[n_class]))\n",
    "\n",
    "\n",
    "#定义RNN网络\n",
    "def RNN(X,weights,biases):\n",
    "    \n",
    "    with tf.name_scope('RNN'):\n",
    "        #input = [batch_size,max_time,n_input]\n",
    "        inputs = tf.reshape(X,[-1,max_time,n_input])\n",
    "        #定义LSTM基本的cell\n",
    "        lstm_cell = rnn.BasicLSTMCell(lstm_size)\n",
    "        #final_state[0] 是 cell state\n",
    "        #final_state[1] 是 hidden_state\n",
    "        outputs,final_state = tf.nn.dynamic_rnn(lstm_cell, inputs, dtype=tf.float32)\n",
    "        results = tf.nn.softmax(tf.matmul(final_state[1],weights) + biases)\n",
    "        return results\n",
    "\n",
    "prediction = RNN(x,weights,biases)\n",
    "\n",
    "\n",
    "#损失函数\n",
    "with tf.name_scope('loss'):\n",
    "    loss =  tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits= prediction,labels=y))\n",
    "\n",
    "#优化器\n",
    "with tf.name_scope('optimizer'):\n",
    "    train_step = tf.train.AdamOptimizer(1e-4).minimize(loss)\n",
    "\n",
    "#结果存在一个bool类型的值中\n",
    "correct_prediction = tf.equal(tf.argmax(prediction,1),tf.argmax(y,1))\n",
    "\n",
    "#求准确率\n",
    "with tf.name_scope('accuracy'):\n",
    "    accuracy =  tf.reduce_mean(tf.cast(correct_prediction,tf.float32))\n",
    "    \n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    \n",
    "    sess.run(init)\n",
    "    \n",
    "    for epoch in range(2):#range(21)\n",
    "        \n",
    "        for batch in range(n_batch):\n",
    "            batch_xs,batch_ys =  mnist.train.next_batch(batch_size)\n",
    "            sess.run(train_step,feed_dict={x:batch_xs,y:batch_ys})\n",
    "        \n",
    "        acc = sess.run(accuracy,feed_dict={x:mnist.test.images,y:mnist.train.labels})\n",
    "        \n",
    "        print('Iter '+ str(epoch) + \" ,     Test accuracy = \" + str(acc) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN对IMDB电影评论建模(成功2019-8-5)+++++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1.准备数据\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing import sequence\n",
    "\n",
    "max_features=10000\n",
    "maxlen = 500\n",
    "batch_size=32\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "PATH='C:\\\\Users\\\\yuli\\\\[机器学习方法]++++++++++++++\\\\IMDB_data\\\\imdb.npz'\n",
    "(input_train,y_train),(input_test,y_test) = imdb.load_data(path=PATH,num_words=max_features)\n",
    "\n",
    "print(len(input_train),'train sequences')\n",
    "print(len(input_test),'test sequences')\n",
    "\n",
    "print('input_train shape:',input_train.shape)\n",
    "print('input_test shape:',input_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Pad sequences (samples x time)')\n",
    "input_train = sequence.pad_sequences(input_train,maxlen=maxlen)\n",
    "input_test = sequence.pad_sequences(input_test,maxlen=maxlen)\n",
    "print('input_train shape:',input_train.shape)\n",
    "print('input_test shape:',input_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2.建立模型并训练\n",
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,SimpleRNN,Activation,Embedding\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(max_features,32))\n",
    "model.add(SimpleRNN(32))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "\n",
    "model.compile(optimizer='rmsprop',\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['acc'])\n",
    "\n",
    "history = model.fit(input_train,\n",
    "                    y_train,\n",
    "                    epochs=5,\n",
    "                    batch_size=128,\n",
    "                    validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#3.绘制曲线\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "def plot_curve(history):\n",
    "    acc = history.history['acc']\n",
    "    val_acc = history.history['val_acc']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    \n",
    "    epochs = range(1,len(acc)+1)\n",
    "    \n",
    "    plt.plot(epochs,acc,'bo',label='Training acc')\n",
    "    plt.plot(epochs,val_acc,'b',label='Validation acc')\n",
    "    plt.title('Training and validation accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.figure()\n",
    "    \n",
    "    plt.plot(epochs,loss,'bo',label='Training loss')\n",
    "    plt.plot(epochs,val_loss,'b',label='Validation loss')\n",
    "    plt.title('Training and validation loss')\n",
    "    plt.legend()\n",
    "    \n",
    "plot_curve(history)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# kears电影评论（成功2019-8-5）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import Sequential  \n",
    "from keras.layers import Input, Dense, Dropout, Activation\n",
    "from keras.models import Model\n",
    "from keras import optimizers\n",
    "from keras.optimizers import SGD\n",
    "from keras.datasets import imdb\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.vis_utils import plot_model\n",
    "\n",
    "tBatchSize = 512\n",
    "Epochs = 10\n",
    "\n",
    "model = Sequential() \n",
    "\n",
    "model.add(Dense(500,input_shape=(10000,))) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5)) \n",
    " \n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(100)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(16)) \n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.1))\n",
    " \n",
    "model.add(Dense(1)) \n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "plot_model(model,to_file = 'model.png',show_shapes = True)\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer=optimizers.RMSprop(lr = 0.001),metrics=['acc'])\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def vectorize_sequences(sequences,dimention = 10000):\n",
    "    results = np.zeros((len(sequences),dimention))\n",
    "    for i,sequence in enumerate(sequences):\n",
    "        results[i,sequence] = 1.\n",
    "    return results\n",
    "\n",
    " #load data\n",
    "PATH='C:\\\\Users\\\\yuli\\\\[机器学习方法]++++++++++++++\\\\IMDB_data\\\\imdb.npz'\n",
    "(X_train, y_train), (X_test, y_test) = imdb.load_data(path=PATH,num_words = 10000)\n",
    "\n",
    "print('Loading data...')\n",
    "\n",
    "X_train = vectorize_sequences(X_train)\n",
    "X_test = vectorize_sequences(X_test)\n",
    "\n",
    "y_train = np.asarray(y_train).astype('float32')\n",
    "y_test = np.asarray(y_test).astype('float32')\n",
    "\n",
    "history = model.fit(X_train, y_train, batch_size=tBatchSize, epochs=Epochs, verbose=2,shuffle=True, validation_split=0.3)\n",
    "\n",
    "score = model.evaluate(X_test,y_test, batch_size=tBatchSize)\n",
    "print(\"The score:\",score[0])\n",
    "print(\"Tne accuracy:\",score[1])\n",
    "\n",
    "history_dic = history.history\n",
    "loss_values = history_dic['loss']\n",
    "val_loss_values = history_dic['val_loss']\n",
    "acc = history_dic['acc']\n",
    "val_acc = history_dic['val_acc']\n",
    "\n",
    "epochs = range(1,len(loss_values)+1)\n",
    "fig = plt.figure()\n",
    "\n",
    "ax1 = fig.add_subplot(111)\n",
    "ax1.plot(epochs,loss_values,'bo',label = 'Training loss')\n",
    "ax1.plot(epochs,val_loss_values,'b',label = 'Validation loss')\n",
    "plt.title(\"Training and validstion loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(bbox_to_anchor=(1,0),loc = 3,borderaxespad = 0)\n",
    "plt.show()\n",
    "'''\n",
    "ax2 = fig.add_subplot(111)\n",
    "ax2.plot(epochs,acc,'bo',label = 'Training acc')\n",
    "ax2.plot(epochs,val_acc,'b',label = 'Validation acc')\n",
    "plt.title(\"Training and validstion accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend(bbox_to_anchor=(1,0),loc = 3,borderaxespad = 0)\n",
    "\n",
    "plt.show()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "334.21px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
